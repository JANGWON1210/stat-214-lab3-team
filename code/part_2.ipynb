{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Modeling and Evaluation\n",
    "\n",
    "In this section, we use the lagged embeddings created in Part 1 (Bag-of-Words, Word2Vec, and GloVe) to build predictive models of brain activity using fMRI voxel data.\n",
    "\n",
    "We follow these key steps:\n",
    "1. Load and validate the preprocessed embeddings (X) for each story.\n",
    "2. Load corresponding fMRI response matrices (Y) for each subject.\n",
    "3. Fit a ridge regression model for each embedding type to predict Y from X.\n",
    "4. Compute the mean correlation coefficient (CC) across all voxels to evaluate model performance.\n",
    "5. Save the trained models and the evaluation metrics for further analysis.\n",
    "\n",
    "Each embedding is evaluated on the same story to ensure consistency across comparisons.\n",
    "This part prepares the ground for more advanced evaluation, such as voxel-level analysis and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's load the data and check they are in the right format by checking story timepoint consistency across embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking story timepoint consistency across embeddings...\n",
      "\n",
      "All stories have consistent timepoint lengths across embeddings.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def load_embeddings(data_dir):\n",
    "    \"\"\"Load all three types of embedding data into a dictionary.\"\"\"\n",
    "    X_bow = joblib.load(data_dir / \"X_lagged_BoW.joblib\")\n",
    "    with open(data_dir / \"X_lagged_W2V.pkl\", \"rb\") as f:\n",
    "        X_w2v = pickle.load(f)\n",
    "    with open(data_dir / \"X_lagged_GloVe.pkl\", \"rb\") as f:\n",
    "        X_glove = pickle.load(f)\n",
    "    return {\"BoW\": X_bow, \"Word2Vec\": X_w2v, \"GloVe\": X_glove}\n",
    "\n",
    "def check_timepoint_consistency(embeddings):\n",
    "    \"\"\"Check if timepoint lengths match across BoW, Word2Vec, and GloVe for each story.\"\"\"\n",
    "    print(\"Checking story timepoint consistency across embeddings...\\n\")\n",
    "    story_set = set(embeddings[\"BoW\"]) & set(embeddings[\"Word2Vec\"]) & set(embeddings[\"GloVe\"])\n",
    "    mismatches = []\n",
    "\n",
    "    for story in sorted(story_set):\n",
    "        l_bow = embeddings[\"BoW\"][story].shape[0]\n",
    "        l_w2v = embeddings[\"Word2Vec\"][story].shape[0]\n",
    "        l_glove = embeddings[\"GloVe\"][story].shape[0]\n",
    "\n",
    "        if not (l_bow == l_w2v == l_glove):\n",
    "            mismatches.append((story, l_bow, l_w2v, l_glove))\n",
    "\n",
    "    if mismatches:\n",
    "        print(\"Found mismatches in the following stories:\")\n",
    "        for story, l_b, l_w, l_g in mismatches:\n",
    "            print(f\"- {story}: BoW={l_b}, Word2Vec={l_w}, GloVe={l_g}\")\n",
    "    else:\n",
    "        print(\"All stories have consistent timepoint lengths across embeddings.\")\n",
    "\n",
    "# === Run check ===\n",
    "DATA_DIR = Path(\"../data\")\n",
    "embeddings = load_embeddings(DATA_DIR)\n",
    "check_timepoint_consistency(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# -------------------------------\n",
    "# Part1 결과 로딩 (이미 생성되었다고 가정)\n",
    "# -------------------------------\n",
    "def load_embeddings(data_dir):\n",
    "    \"\"\"Load all three types of embedding data into a dictionary.\n",
    "    data_dir: Path to the directory with part1 embedding result files.\"\"\"\n",
    "    X_bow = joblib.load(data_dir / \"X_lagged_BoW.joblib\")\n",
    "    with open(data_dir / \"X_lagged_W2V.pkl\", \"rb\") as f:\n",
    "        X_w2v = pickle.load(f)\n",
    "    with open(data_dir / \"X_lagged_GloVe.pkl\", \"rb\") as f:\n",
    "        X_glove = pickle.load(f)\n",
    "    return {\"BoW\": X_bow, \"Word2Vec\": X_w2v, \"GloVe\": X_glove}\n",
    "\n",
    "# -------------------------------\n",
    "# fMRI 데이터 로딩\n",
    "# -------------------------------\n",
    "def load_fmri(subject_dir):\n",
    "    \"\"\"\n",
    "    subject_dir: Path to subject folder containing .npy files for each story.\n",
    "    Returns a dictionary mapping story names to fMRI response matrices.\n",
    "    \"\"\"\n",
    "    fmri_data = {}\n",
    "    for file in subject_dir.glob(\"*.npy\"):\n",
    "        story = file.stem  # 파일 이름에서 확장자 제거\n",
    "        fmri_data[story] = np.load(file)\n",
    "    return fmri_data\n",
    "\n",
    "# -------------------------------\n",
    "# 스토리 분할 (70% training, 30% testing)\n",
    "# -------------------------------\n",
    "def split_stories(story_list, train_ratio=0.7, random_state=42):\n",
    "    \"\"\"\n",
    "    story_list: list of story identifiers\n",
    "    Returns: train_stories, test_stories (둘 다 리스트)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    stories = sorted(story_list)\n",
    "    np.random.shuffle(stories)\n",
    "    split_idx = int(len(stories) * train_ratio)\n",
    "    return stories[:split_idx], stories[split_idx:]\n",
    "\n",
    "# -------------------------------\n",
    "# Data Preparation: X (embedding)와 Y (fMRI) 데이터 연결\n",
    "# -------------------------------\n",
    "def prepare_data(stories, embedding_dict, fmri_dict):\n",
    "    \"\"\"\n",
    "    stories: list of story names to include.\n",
    "    embedding_dict: dictionary for one embedding method (e.g., embeddings[\"BoW\"])\n",
    "    fmri_dict: subject의 fMRI 데이터 dictionary.\n",
    "    Returns: X, Y where data for each story are concatenated along time axis.\n",
    "    \"\"\"\n",
    "    X_list, Y_list = [], []\n",
    "    for story in stories:\n",
    "        if story in embedding_dict and story in fmri_dict:\n",
    "            X_list.append(embedding_dict[story])\n",
    "            Y_list.append(fmri_dict[story])\n",
    "        else:\n",
    "            print(f\"Warning: Story {story} is missing in embeddings or fMRI data.\")\n",
    "    if len(X_list) == 0 or len(Y_list) == 0:\n",
    "        raise ValueError(\"No overlapping stories found!\")\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    Y = np.concatenate(Y_list, axis=0)\n",
    "    return X, Y\n",
    "\n",
    "# -------------------------------\n",
    "# Voxel 단위 상관계수 (CC) 계산 함수\n",
    "# -------------------------------\n",
    "def compute_voxel_cc(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Y_true, Y_pred: shape (N_time, V_voxels)\n",
    "    Returns: 1D numpy array of CC (길이 V)\n",
    "    \"\"\"\n",
    "    n_voxels = Y_true.shape[1]\n",
    "    voxel_ccs = []\n",
    "    for i in range(n_voxels):\n",
    "        true_voxel = Y_true[:, i]\n",
    "        pred_voxel = Y_pred[:, i]\n",
    "        if np.std(true_voxel) == 0 or np.std(pred_voxel) == 0:\n",
    "            cc = 0.0\n",
    "        else:\n",
    "            cc = np.corrcoef(true_voxel, pred_voxel)[0, 1]\n",
    "        voxel_ccs.append(cc)\n",
    "    return np.array(voxel_ccs)\n",
    "\n",
    "# -------------------------------\n",
    "# Ridge regression 모델 학습 및 평가 함수\n",
    "# -------------------------------\n",
    "def train_and_evaluate(X_train, Y_train, X_test, Y_test, subject, emb_name):\n",
    "    \"\"\"\n",
    "    주어진 학습/테스트 데이터를 바탕으로 Ridge 회귀 모형 학습 및 평가\n",
    "    - 내부 교차 검증으로 최적 alpha 선택 (RidgeCV 이용)\n",
    "    - 모델 저장 (.pkl 파일, results 폴더에 저장)\n",
    "    - 테스트 set에 대해 voxel별 CC를 계산하고 요약 통계량을 반환\n",
    "    \"\"\"\n",
    "    # 하이퍼파라미터: alpha 후보 값 (필요에 따라 조정)\n",
    "    alphas = np.logspace(1, 3, 10)\n",
    "    ridge = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=5)\n",
    "    ridge.fit(X_train, Y_train)\n",
    "\n",
    "    # 모델 저장 (ridge_utils의 코드 활용할 수 있음  참고)\n",
    "    results_dir = Path(\"results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    model_filename = results_dir / f\"ridge_{subject}_{emb_name}.pkl\"\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(ridge, f)\n",
    "    print(f\"Saved model for {subject} - {emb_name} to {model_filename}\")\n",
    "\n",
    "    # 테스트 데이터에 대한 예측 및 평가\n",
    "    Y_pred = ridge.predict(X_test)\n",
    "    voxel_ccs = compute_voxel_cc(Y_test, Y_pred)\n",
    "    mean_cc = np.mean(voxel_ccs)\n",
    "    median_cc = np.median(voxel_ccs)\n",
    "    top1_cc = np.percentile(voxel_ccs, 99)  # 상위 1% (99번째 백분위)\n",
    "    top5_cc = np.percentile(voxel_ccs, 95)  # 상위 5% (95번째 백분위)\n",
    "    metrics = {\n",
    "        \"mean_cc\": mean_cc,\n",
    "        \"median_cc\": median_cc,\n",
    "        \"top1_cc\": top1_cc,\n",
    "        \"top5_cc\": top5_cc,\n",
    "        \"voxel_ccs\": voxel_ccs\n",
    "    }\n",
    "    print(f\"[{subject} - {emb_name}] Evaluation Metrics: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "# -------------------------------\n",
    "# CC 분포 플롯 생성 함수\n",
    "# -------------------------------\n",
    "def plot_cc_distribution(voxel_ccs, subject, emb_name):\n",
    "    plt.figure()\n",
    "    plt.hist(voxel_ccs, bins=50)\n",
    "    plt.xlabel(\"Correlation Coefficient (CC)\")\n",
    "    plt.ylabel(\"Number of Voxels\")\n",
    "    plt.title(f\"CC Distribution for {subject} ({emb_name})\")\n",
    "    plot_filename = f\"results/cc_distribution_{subject}_{emb_name}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved distribution plot to {plot_filename}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 메인 파이프라인\n",
    "# -------------------------------\n",
    "def main():\n",
    "    # 경로 설정\n",
    "    data_dir = Path(\"../data\")  # 파트1의 embedding 결과 파일들이 위치\n",
    "    subject2_dir = Path(\"../../tmp_ondemand_ocean_mth240012p_symlink/shared/data/subject2\")\n",
    "    subject3_dir = Path(\"../../tmp_ondemand_ocean_mth240012p_symlink/shared/data/subject3\")\n",
    "    \n",
    "    # 파트1 결과 (embedding)을 로딩\n",
    "    embeddings = load_embeddings(data_dir)\n",
    "    # embeddings는 {\"BoW\": {story: embedding_matrix, ...}, \"Word2Vec\": ..., \"GloVe\": ...} 형태\n",
    "    \n",
    "    # fMRI 데이터 로딩 (각 subject별로)\n",
    "    fmri_subject2 = load_fmri(subject2_dir)\n",
    "    fmri_subject3 = load_fmri(subject3_dir)\n",
    "    \n",
    "    # 두 subject와 embeddings에 모두 포함된 story들을 선택 (동일 split 적용)\n",
    "    stories_subject2 = set(embeddings[\"BoW\"].keys()) & set(fmri_subject2.keys())\n",
    "    stories_subject3 = set(embeddings[\"BoW\"].keys()) & set(fmri_subject3.keys())\n",
    "    common_stories = sorted(list(stories_subject2 & stories_subject3))\n",
    "    print(f\"Total common stories across embeddings and both subjects: {len(common_stories)}\")\n",
    "    \n",
    "    # 70% training / 30% test split (동일한 난수 seed 사용)\n",
    "    train_stories, test_stories = split_stories(common_stories, train_ratio=0.7, random_state=42)\n",
    "    print(f\"Training stories: {len(train_stories)}; Testing stories: {len(test_stories)}\")\n",
    "    \n",
    "    subjects = {\"subject2\": fmri_subject2, \"subject3\": fmri_subject3}\n",
    "    results = {}\n",
    "    \n",
    "    # 각 subject 및 각 embedding 방법에 대해 모델 학습 및 평가 실시\n",
    "    for subject, fmri_data in subjects.items():\n",
    "        results[subject] = {}\n",
    "        # for emb_name, emb_dict in embeddings.items():\n",
    "        for emb_name in [\"BoW\"]:\n",
    "            emb_dict = embeddings[emb_name]\n",
    "            print(f\"Processing {subject} with {emb_name} embeddings...\")\n",
    "            X_train, Y_train = prepare_data(train_stories, emb_dict, fmri_data)\n",
    "            X_test, Y_test = prepare_data(test_stories, emb_dict, fmri_data)\n",
    "            metrics = train_and_evaluate(X_train, Y_train, X_test, Y_test, subject, emb_name)\n",
    "            results[subject][emb_name] = metrics\n",
    "\n",
    "    # 각 embedding에 대해 두 subject의 mean_cc 평균을 계산하여 최고 성능 embedding 선택\n",
    "    best_emb = None\n",
    "    best_mean_cc = -np.inf\n",
    "    for emb_name in embeddings.keys():\n",
    "        avg_mean_cc = np.mean([results[subj][emb_name][\"mean_cc\"] for subj in subjects])\n",
    "        print(f\"Average mean CC for {emb_name}: {avg_mean_cc}\")\n",
    "        if avg_mean_cc > best_mean_cc:\n",
    "            best_mean_cc = avg_mean_cc\n",
    "            best_emb = emb_name\n",
    "    print(f\"Best performing embedding overall: {best_emb} with average mean CC {best_mean_cc}\")\n",
    "    \n",
    "    # 최고 성능 embedding에 대해 각 subject 별 CC 분포 플롯 생성 (추가 분석)\n",
    "    for subject in subjects.keys():\n",
    "        voxel_ccs = results[subject][best_emb][\"voxel_ccs\"]\n",
    "        plot_cc_distribution(voxel_ccs, subject, best_emb)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
