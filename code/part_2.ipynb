{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Modeling and Evaluation\n",
    "\n",
    "In this section, we use the lagged embeddings created in Part 1 (Bag-of-Words, Word2Vec, and GloVe) to build predictive models of brain activity using fMRI voxel data.\n",
    "\n",
    "We follow these key steps:\n",
    "1. Load and validate the preprocessed embeddings (X) for each story.\n",
    "2. Load corresponding fMRI response matrices (Y) for each subject.\n",
    "3. Fit a ridge regression model for each embedding type to predict Y from X.\n",
    "4. Compute the mean correlation coefficient (CC) across all voxels to evaluate model performance.\n",
    "5. Save the trained models and the evaluation metrics for further analysis.\n",
    "\n",
    "Each embedding is evaluated on the same story to ensure consistency across comparisons.\n",
    "This part prepares the ground for more advanced evaluation, such as voxel-level analysis and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's load the data and check they are in the right format by checking story timepoint consistency across embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking story timepoint consistency across embeddings...\n",
      "\n",
      "All stories have consistent timepoint lengths across embeddings.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def load_embeddings(data_dir):\n",
    "    \"\"\"Load all three types of embedding data into a dictionary.\"\"\"\n",
    "    X_bow = joblib.load(data_dir / \"X_lagged_BoW.joblib\")\n",
    "    with open(data_dir / \"X_lagged_W2V.pkl\", \"rb\") as f:\n",
    "        X_w2v = pickle.load(f)\n",
    "    with open(data_dir / \"X_lagged_GloVe.pkl\", \"rb\") as f:\n",
    "        X_glove = pickle.load(f)\n",
    "    return {\"BoW\": X_bow, \"Word2Vec\": X_w2v, \"GloVe\": X_glove}\n",
    "\n",
    "def check_timepoint_consistency(embeddings):\n",
    "    \"\"\"Check if timepoint lengths match across BoW, Word2Vec, and GloVe for each story.\"\"\"\n",
    "    print(\"Checking story timepoint consistency across embeddings...\\n\")\n",
    "    story_set = set(embeddings[\"BoW\"]) & set(embeddings[\"Word2Vec\"]) & set(embeddings[\"GloVe\"])\n",
    "    mismatches = []\n",
    "\n",
    "    for story in sorted(story_set):\n",
    "        l_bow = embeddings[\"BoW\"][story].shape[0]\n",
    "        l_w2v = embeddings[\"Word2Vec\"][story].shape[0]\n",
    "        l_glove = embeddings[\"GloVe\"][story].shape[0]\n",
    "\n",
    "        if not (l_bow == l_w2v == l_glove):\n",
    "            mismatches.append((story, l_bow, l_w2v, l_glove))\n",
    "\n",
    "    if mismatches:\n",
    "        print(\"Found mismatches in the following stories:\")\n",
    "        for story, l_b, l_w, l_g in mismatches:\n",
    "            print(f\"- {story}: BoW={l_b}, Word2Vec={l_w}, GloVe={l_g}\")\n",
    "    else:\n",
    "        print(\"All stories have consistent timepoint lengths across embeddings.\")\n",
    "\n",
    "# === Run check ===\n",
    "DATA_DIR = Path(\"../data\")\n",
    "embeddings = load_embeddings(DATA_DIR)\n",
    "check_timepoint_consistency(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total common stories across embeddings and both subjects: 101\n",
      "Training stories: 70; Testing stories: 31\n",
      "Processing subject2 with BoW embeddings...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Append the ridge_utils folder to the module search path so we can import from it\n",
    "sys.path.append(\"./ridge_utils\")\n",
    "from ridge import bootstrap_ridge  # Use the bootstrap function defined in ridge.py\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Define a run_ridge() wrapper function for GPU-accelerated ridge regression.\n",
    "# This function wraps bootstrap_ridge() to select optimal alpha parameters via cross-validation,\n",
    "# compute the regression weights, and evaluate the test set performance.\n",
    "# ------------------------------------------------------------------------------\n",
    "def run_ridge(X_train, Y_train, X_test, Y_test, alphas=None, cv=15, chunklen=10, nchunks=5):\n",
    "    \"\"\"\n",
    "    Wrapper function for GPU-accelerated ridge regression using bootstrap_ridge.\n",
    "    Returns a tuple (wt, metrics) where:\n",
    "      - wt: Regression weights (model) of shape (features, voxels)\n",
    "      - metrics: A dictionary with keys \"mean_cc\", \"median_cc\", \"top1_cc\",\n",
    "                 \"top5_cc\", and \"voxel_ccs\" containing evaluation metrics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, Y_train : array_like\n",
    "        Training stimulus and response data.\n",
    "    X_test, Y_test : array_like\n",
    "        Test stimulus and response data.\n",
    "    alphas : array_like or None, optional\n",
    "        Candidate ridge regularization parameters. If None, defaults to np.logspace(1, 3, 10).\n",
    "    cv : int, optional\n",
    "        Number of bootstrap cross-validation runs.\n",
    "    chunklen : int, optional\n",
    "        Length of each hold-out chunk (must be tuned to your data).\n",
    "    nchunks : int, optional\n",
    "        Number of chunks to hold out per bootstrap run.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    wt : array_like, shape (n_features, n_voxels)\n",
    "        Trained ridge regression weights.\n",
    "    metrics : dict\n",
    "        Dictionary of evaluation metrics on the test set.\n",
    "    \"\"\"\n",
    "    if alphas is None:\n",
    "        alphas = np.logspace(1, 3, 10)\n",
    "\n",
    "    # Define a simple z-score function\n",
    "    def zs(v):\n",
    "        std = v.std(0)\n",
    "        std[std == 0] = 1\n",
    "        return (v - v.mean(0)) / std\n",
    "    \n",
    "    # Z-score the training and test data\n",
    "    X_train_z = zs(X_train)\n",
    "    Y_train_z = zs(Y_train)\n",
    "    X_test_z  = zs(X_test)\n",
    "    Y_test_z  = zs(Y_test)\n",
    "    \n",
    "    # Run ridge regression with bootstrap cross-validation to determine optimal alphas and weights\n",
    "    wt, corrs, valphas, _, _ = bootstrap_ridge(\n",
    "        X_train_z, Y_train_z, X_test_z, Y_test_z,\n",
    "        alphas=np.array(alphas),\n",
    "        nboots=cv,\n",
    "        chunklen=chunklen,\n",
    "        nchunks=nchunks,\n",
    "        use_corr=True,\n",
    "        normalpha=False,\n",
    "        return_wt=True\n",
    "    )\n",
    "    \n",
    "    # Compute test predictions and evaluate voxel-wise correlations\n",
    "    Y_pred = np.dot(X_test_z, wt)\n",
    "    voxel_ccs = np.array([\n",
    "        np.corrcoef(Y_test_z[:, i], Y_pred[:, i])[0, 1] if np.std(Y_pred[:, i]) > 0 else 0\n",
    "        for i in range(Y_test_z.shape[1])\n",
    "    ])\n",
    "    metrics = {\n",
    "        \"mean_cc\": np.mean(voxel_ccs),\n",
    "        \"median_cc\": np.median(voxel_ccs),\n",
    "        \"top1_cc\": np.percentile(voxel_ccs, 99),\n",
    "        \"top5_cc\": np.percentile(voxel_ccs, 95),\n",
    "        \"voxel_ccs\": voxel_ccs\n",
    "    }\n",
    "    \n",
    "    return wt, metrics\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Load precomputed embedding results (from Part 1)\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_embeddings(data_dir):\n",
    "    \"\"\"\n",
    "    Load all three types of embedding data into a dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : Path\n",
    "        Path to the directory containing Part 1 embedding result files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    embeddings : dict\n",
    "        Dictionary mapping embedding names (\"BoW\", \"Word2Vec\", \"GloVe\") to their corresponding matrices.\n",
    "    \"\"\"\n",
    "    X_bow = joblib.load(data_dir / \"X_lagged_BoW.joblib\")\n",
    "    with open(data_dir / \"X_lagged_W2V.pkl\", \"rb\") as f:\n",
    "        X_w2v = pickle.load(f)\n",
    "    with open(data_dir / \"X_lagged_GloVe.pkl\", \"rb\") as f:\n",
    "        X_glove = pickle.load(f)\n",
    "    return {\"BoW\": X_bow, \"Word2Vec\": X_w2v, \"GloVe\": X_glove}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Load fMRI data from a subject's directory\n",
    "# ------------------------------------------------------------------------------\n",
    "def load_fmri(subject_dir):\n",
    "    \"\"\"\n",
    "    Load fMRI data from .npy files in a subject folder.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_dir : Path\n",
    "        Path to the folder containing .npy files for each story.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fmri_data : dict\n",
    "        Dictionary mapping story names to fMRI response matrices.\n",
    "    \"\"\"\n",
    "    fmri_data = {}\n",
    "    for file in subject_dir.glob(\"*.npy\"):\n",
    "        story = file.stem  # Remove file extension\n",
    "        fmri_data[story] = np.load(file)\n",
    "    return fmri_data\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Split stories into training and testing sets (70% / 30%)\n",
    "# ------------------------------------------------------------------------------\n",
    "def split_stories(story_list, train_ratio=0.7, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a list of story identifiers into training and testing sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    story_list : list\n",
    "        List of story identifiers.\n",
    "    train_ratio : float, optional\n",
    "        Proportion of stories to use for training.\n",
    "    random_state : int, optional\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_stories : list\n",
    "        List of training story identifiers.\n",
    "    test_stories : list\n",
    "        List of testing story identifiers.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    stories = sorted(story_list)\n",
    "    np.random.shuffle(stories)\n",
    "    split_idx = int(len(stories) * train_ratio)\n",
    "    return stories[:split_idx], stories[split_idx:]\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Prepare data by concatenating embedding (X) and fMRI (Y) data across stories\n",
    "# ------------------------------------------------------------------------------\n",
    "def prepare_data(stories, embedding_dict, fmri_dict):\n",
    "    \"\"\"\n",
    "    Concatenate data across a list of stories.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stories : list\n",
    "        List of story names to include.\n",
    "    embedding_dict : dict\n",
    "        Dictionary for one embedding method.\n",
    "    fmri_dict : dict\n",
    "        Subjectâ€™s fMRI data dictionary.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : array_like\n",
    "        Concatenated embedding matrix along the time axis.\n",
    "    Y : array_like\n",
    "        Concatenated fMRI response matrix along the time axis.\n",
    "    \"\"\"\n",
    "    X_list, Y_list = [], []\n",
    "    for story in stories:\n",
    "        if story in embedding_dict and story in fmri_dict:\n",
    "            X_list.append(embedding_dict[story])\n",
    "            Y_list.append(fmri_dict[story])\n",
    "        else:\n",
    "            print(f\"Warning: Story {story} is missing in embeddings or fMRI data.\")\n",
    "    if len(X_list) == 0 or len(Y_list) == 0:\n",
    "        raise ValueError(\"No overlapping stories found!\")\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    Y = np.concatenate(Y_list, axis=0)\n",
    "    return X, Y\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Compute voxel-wise correlation coefficient (CC)\n",
    "# ------------------------------------------------------------------------------\n",
    "def compute_voxel_cc(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Compute the correlation coefficient for each voxel between true and predicted responses.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_true, Y_pred : array_like\n",
    "        Arrays of shape (N_time, V_voxels).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    voxel_ccs : array_like\n",
    "        1D numpy array of correlation coefficients (length V).\n",
    "    \"\"\"\n",
    "    n_voxels = Y_true.shape[1]\n",
    "    voxel_ccs = []\n",
    "    for i in range(n_voxels):\n",
    "        true_voxel = Y_true[:, i]\n",
    "        pred_voxel = Y_pred[:, i]\n",
    "        if np.std(true_voxel) == 0 or np.std(pred_voxel) == 0:\n",
    "            cc = 0.0\n",
    "        else:\n",
    "            cc = np.corrcoef(true_voxel, pred_voxel)[0, 1]\n",
    "        voxel_ccs.append(cc)\n",
    "    return np.array(voxel_ccs)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Train and evaluate GPU-accelerated ridge regression model\n",
    "# ------------------------------------------------------------------------------\n",
    "def train_and_evaluate_gpu(X_train, Y_train, X_test, Y_test, subject, emb_name):\n",
    "    \"\"\"\n",
    "    Train a ridge regression model using GPU acceleration.\n",
    "    \n",
    "    This function selects the optimal alpha via internal cross-validation\n",
    "    using run_ridge(), saves the trained model as a .pkl file under the results folder,\n",
    "    and returns evaluation metrics computed on the test set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, Y_train : array_like\n",
    "        Training data.\n",
    "    X_test, Y_test : array_like\n",
    "        Test data.\n",
    "    subject : str\n",
    "        Subject identifier.\n",
    "    emb_name : str\n",
    "        Embedding method name (e.g., \"BoW\").\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Evaluation metrics including mean CC, median CC, top 1%, and top 5% percentile CC.\n",
    "    \"\"\"\n",
    "    # Define candidate alpha values\n",
    "    alphas = np.logspace(1, 3, 10)\n",
    "    # Run the GPU-accelerated ridge regression via run_ridge()\n",
    "    model, metrics = run_ridge(X_train, Y_train, X_test, Y_test, alphas=alphas, cv=5)\n",
    "    \n",
    "    # Save the trained model into the results folder as a .pkl file\n",
    "    results_dir = Path(\"results\")\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    model_filename = results_dir / f\"ridge_{subject}_{emb_name}.pkl\"\n",
    "    with open(model_filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Saved model for {subject} - {emb_name} to {model_filename}\")\n",
    "    print(f\"[{subject} - {emb_name}] Evaluation Metrics: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Plot the distribution of voxel-wise correlation coefficients (CC)\n",
    "# ------------------------------------------------------------------------------\n",
    "def plot_cc_distribution(voxel_ccs, subject, emb_name):\n",
    "    \"\"\"\n",
    "    Plot and save a histogram of the correlation coefficients across voxels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    voxel_ccs : array_like\n",
    "        1D array of correlation coefficients.\n",
    "    subject : str\n",
    "        Subject identifier.\n",
    "    emb_name : str\n",
    "        Embedding method name.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.hist(voxel_ccs, bins=50)\n",
    "    plt.xlabel(\"Correlation Coefficient (CC)\")\n",
    "    plt.ylabel(\"Number of Voxels\")\n",
    "    plt.title(f\"CC Distribution for {subject} ({emb_name})\")\n",
    "    plot_filename = f\"results/cc_distribution_{subject}_{emb_name}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved distribution plot to {plot_filename}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Main pipeline\n",
    "# ------------------------------------------------------------------------------\n",
    "def main():\n",
    "    # Set directories for embedding data (Part 1) and fMRI data for each subject\n",
    "    data_dir = Path(\"../data\")  # Precomputed embedding files from Part 1\n",
    "    subject2_dir = Path(\"../../tmp_ondemand_ocean_mth240012p_symlink/shared/data/subject2\")\n",
    "    subject3_dir = Path(\"../../tmp_ondemand_ocean_mth240012p_symlink/shared/data/subject3\")\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeddings = load_embeddings(data_dir)\n",
    "    \n",
    "    # Load fMRI data for each subject\n",
    "    fmri_subject2 = load_fmri(subject2_dir)\n",
    "    fmri_subject3 = load_fmri(subject3_dir)\n",
    "    \n",
    "    # Identify common stories available in both embeddings and fMRI data\n",
    "    stories_subject2 = set(embeddings[\"BoW\"].keys()) & set(fmri_subject2.keys())\n",
    "    stories_subject3 = set(embeddings[\"BoW\"].keys()) & set(fmri_subject3.keys())\n",
    "    common_stories = sorted(list(stories_subject2 & stories_subject3))\n",
    "    print(f\"Total common stories across embeddings and both subjects: {len(common_stories)}\")\n",
    "    \n",
    "    # Split the stories into training (70%) and testing (30%) sets\n",
    "    train_stories, test_stories = split_stories(common_stories, train_ratio=0.7, random_state=42)\n",
    "    print(f\"Training stories: {len(train_stories)}; Testing stories: {len(test_stories)}\")\n",
    "    \n",
    "    subjects = {\"subject2\": fmri_subject2, \"subject3\": fmri_subject3}\n",
    "    results = {}\n",
    "    \n",
    "    # For each subject and for a selected embedding type (here, \"BoW\"), train and evaluate the model\n",
    "    for subject, fmri_data in subjects.items():\n",
    "        results[subject] = {}\n",
    "        for emb_name in [\"BoW\"]:   # [\"BoW\", \"Word2Vec\", \"GloVe\"]\n",
    "            emb_dict = embeddings[emb_name]\n",
    "            print(f\"Processing {subject} with {emb_name} embeddings...\")\n",
    "            X_train, Y_train = prepare_data(train_stories, emb_dict, fmri_data)\n",
    "            X_test, Y_test = prepare_data(test_stories, emb_dict, fmri_data)\n",
    "            metrics = train_and_evaluate_gpu(X_train, Y_train, X_test, Y_test, subject, emb_name)\n",
    "            results[subject][emb_name] = metrics\n",
    "    \n",
    "    # Select the best performing embedding based on the average mean CC across both subjects\n",
    "    best_emb = None\n",
    "    best_mean_cc = -np.inf\n",
    "    for emb_name in embeddings.keys():\n",
    "        if emb_name in results[\"subject2\"] and emb_name in results[\"subject3\"]:\n",
    "            avg_mean_cc = np.mean([results[subj][emb_name][\"mean_cc\"] for subj in subjects])\n",
    "            print(f\"Average mean CC for {emb_name}: {avg_mean_cc}\")\n",
    "            if avg_mean_cc > best_mean_cc:\n",
    "                best_mean_cc = avg_mean_cc\n",
    "                best_emb = emb_name\n",
    "    print(f\"Best performing embedding overall: {best_emb} with average mean CC {best_mean_cc}\")\n",
    "    \n",
    "    # For the best embedding, plot the distribution of voxel-wise CC for each subject\n",
    "    for subject in subjects.keys():\n",
    "        voxel_ccs = results[subject][best_emb][\"voxel_ccs\"]\n",
    "        plot_cc_distribution(voxel_ccs, subject, best_emb)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metrics_filename = results_dir / f\"metrics_{subject}_{emb_name}.json\"\n",
    "with open(metrics_filename, \"w\") as f:\n",
    "    json.dump({k: (v.tolist() if isinstance(v, np.ndarray) else v) for k, v in metrics.items()}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_214",
   "language": "python",
   "name": "env_214"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
