{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Modeling and Evaluation\n",
    "\n",
    "In this section, we use the lagged embeddings created in Part 1 (Bag-of-Words, Word2Vec, and GloVe) to build predictive models of brain activity using fMRI voxel data.\n",
    "\n",
    "We follow these key steps:\n",
    "1. Load and validate the preprocessed embeddings (X) for each story.\n",
    "2. Load corresponding fMRI response matrices (Y) for each subject.\n",
    "3. Fit a ridge regression model for each embedding type to predict Y from X.\n",
    "4. Compute the mean correlation coefficient (CC) across all voxels to evaluate model performance.\n",
    "5. Save the trained models and the evaluation metrics for further analysis.\n",
    "\n",
    "Each embedding is evaluated on the same story to ensure consistency across comparisons.\n",
    "This part prepares the ground for more advanced evaluation, such as voxel-level analysis and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's load the data and check they are in the right format by checking story timepoint consistency across embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking story timepoint consistency across embeddings...\n",
      "\n",
      "All stories have consistent timepoint lengths across embeddings.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Load X_bow, X_w2v, and X_glove\n",
    "def load_embeddings(data_dir):\n",
    "    \"\"\"Load all three types of embedding data into a dictionary.\"\"\"\n",
    "    X_bow = joblib.load(data_dir / \"X_lagged_BoW.joblib\")\n",
    "    with open(data_dir / \"X_lagged_W2V.pkl\", \"rb\") as f:\n",
    "        X_w2v = pickle.load(f)\n",
    "    with open(data_dir / \"X_lagged_GloVe.pkl\", \"rb\") as f:\n",
    "        X_glove = pickle.load(f)\n",
    "    return {\"BoW\": X_bow, \"Word2Vec\": X_w2v, \"GloVe\": X_glove}\n",
    "\n",
    "def check_timepoint_consistency(embeddings):\n",
    "    \"\"\"Check if timepoint lengths match across BoW, Word2Vec, and GloVe for each story.\"\"\"\n",
    "    print(\"Checking story timepoint consistency across embeddings...\\n\")\n",
    "    story_set = set(embeddings[\"BoW\"]) & set(embeddings[\"Word2Vec\"]) & set(embeddings[\"GloVe\"])\n",
    "    mismatches = []\n",
    "\n",
    "    for story in sorted(story_set):\n",
    "        l_bow = embeddings[\"BoW\"][story].shape[0]\n",
    "        l_w2v = embeddings[\"Word2Vec\"][story].shape[0]\n",
    "        l_glove = embeddings[\"GloVe\"][story].shape[0]\n",
    "\n",
    "        if not (l_bow == l_w2v == l_glove):\n",
    "            mismatches.append((story, l_bow, l_w2v, l_glove))\n",
    "\n",
    "    if mismatches:\n",
    "        print(\"Found mismatches in the following stories:\")\n",
    "        for story, l_b, l_w, l_g in mismatches:\n",
    "            print(f\"- {story}: BoW={l_b}, Word2Vec={l_w}, GloVe={l_g}\")\n",
    "    else:\n",
    "        print(\"All stories have consistent timepoint lengths across embeddings.\")\n",
    "\n",
    "# === Run check ===\n",
    "DATA_DIR = Path(\"../data\")\n",
    "embeddings = load_embeddings(DATA_DIR)\n",
    "check_timepoint_consistency(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_subject_y_data(subject_dir):\n",
    "    \"\"\"\n",
    "    Load all .npy files in a directory and return a dict {story: np.ndarray}\n",
    "    \"\"\"\n",
    "    subject_dir = Path(subject_dir)\n",
    "    y_files = list(subject_dir.glob(\"*.npy\"))\n",
    "    print(f\"Loaded\")\n",
    "    return {f.stem: np.load(f, mmap_mode='r') for f in y_files}\n",
    "\n",
    "# Load Subject Y data\n",
    "subject2_y = load_subject_y_data(\"../../tmp_ondemand_ocean_mth240012p_symlink/shared/data/subject2\")\n",
    "subject3_y = load_subject_y_data(\"../../tmp_ondemand_ocean_mth240012p_symlink/shared/data/subject3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of voxels:\", subject2_y[\"tildeath\"].shape[1])\n",
    "print(\"Number of voxels:\", subject3_y[\"tildeath\"].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def split_and_save_XY_data(\n",
    "    X_dicts, subject_y_dicts, save_dir, test_ratio=0.3, seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Split story-level data into training and test sets, and save them with compression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_dicts : dict\n",
    "        Dictionary containing embeddings {\"BoW\": ..., \"Word2Vec\": ..., \"GloVe\": ...}\n",
    "    subject_y_dicts : dict\n",
    "        Dictionary of Y values per subject, e.g., {\"subject2\": {...}, \"subject3\": {...}}\n",
    "    save_dir : str or Path\n",
    "        Directory to save the resulting data files\n",
    "    test_ratio : float\n",
    "        Fraction of stories to assign to test set (default: 0.3)\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Find common story names across all X and Y dictionaries\n",
    "    story_sets = [set(d.keys()) for d in X_dicts.values()] + [set(v.keys()) for v in subject_y_dicts.values()]\n",
    "    common_stories = sorted(set.intersection(*story_sets))\n",
    "\n",
    "    # Randomly split stories into train and test sets\n",
    "    random.seed(seed)\n",
    "    n_test = int(len(common_stories) * test_ratio)\n",
    "    test_stories = sorted(random.sample(common_stories, n_test))\n",
    "    train_stories = sorted(list(set(common_stories) - set(test_stories)))\n",
    "\n",
    "    # Save X_train and X_test for each embedding type (with compression)\n",
    "    for name, Xdict in X_dicts.items():\n",
    "        X_train = np.concatenate([Xdict[s] for s in train_stories], axis=0)\n",
    "        X_test = np.concatenate([Xdict[s] for s in test_stories], axis=0)\n",
    "        joblib.dump(X_train, save_dir / f\"X_train_{name}.joblib\", compress=3)\n",
    "        joblib.dump(X_test, save_dir / f\"X_test_{name}.joblib\", compress=3)\n",
    "\n",
    "    # Save Y_train and Y_test for each subject (compressed npz)\n",
    "    for subject_name, Ydict in subject_y_dicts.items():\n",
    "        Y_train = np.concatenate([Ydict[s] for s in train_stories], axis=0)\n",
    "        Y_test = np.concatenate([Ydict[s] for s in test_stories], axis=0)\n",
    "        np.savez_compressed(save_dir / f\"Y_train_{subject_name}.npz\", Y_train)\n",
    "        np.savez_compressed(save_dir / f\"Y_test_{subject_name}.npz\", Y_test)\n",
    "\n",
    "    print(\"✅ Saved all training and test data.\")\n",
    "    # print(\"Train stories:\", train_stories)\n",
    "    # print(\"Test stories :\", test_stories)\n",
    "\n",
    "    return train_stories, test_stories\n",
    "\n",
    "X_dicts = {\n",
    "    \"BoW\": embeddings[\"BoW\"],\n",
    "    \"Word2Vec\": embeddings[\"Word2Vec\"],\n",
    "    \"GloVe\": embeddings[\"GloVe\"]\n",
    "}\n",
    "\n",
    "subject_y_dicts = {\n",
    "    \"subject2\": subject2_y,\n",
    "    \"subject3\": subject3_y\n",
    "}\n",
    "\n",
    "train_stories, test_stories = split_and_save_XY_data(\n",
    "    X_dicts=X_dicts,\n",
    "    subject_y_dicts=subject_y_dicts,\n",
    "    save_dir=\"../data\",\n",
    "    test_ratio=0.3,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def print_mismatched_x_y(embeddings, y_dir):\n",
    "    y_dir = Path(y_dir)\n",
    "    y_files = list(y_dir.glob(\"*.npy\"))\n",
    "    \n",
    "    y_dict = {f.stem: np.load(f, mmap_mode='r') for f in y_files}\n",
    "\n",
    "    stories = set(embeddings[\"BoW\"]) & set(y_dict)\n",
    "\n",
    "    mismatch_found = False\n",
    "\n",
    "    for story in sorted(stories):\n",
    "        x_tr = embeddings[\"BoW\"][story].shape[0]\n",
    "        y_tr = y_dict[story].shape[0]\n",
    "        if x_tr != y_tr:\n",
    "            print(f\"❌ {story}: X_TR={x_tr}, Y_TR={y_tr}\")\n",
    "            mismatch_found = True\n",
    "\n",
    "    if not mismatch_found:\n",
    "        print(\"✅ All stories match\")\n",
    "\n",
    "print_mismatched_x_y(embeddings, \"../../tmp_ondemand_ocean_mth240012p_symlink/shared/data/subject2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is structured to support efficient and modular execution, primarily due to the large size of the datasets involved. Since memory is a key constraint—especially with high-dimensional BoW features and voxel responses—the script avoids loading all data at once. Instead, it processes each subject and embedding combination one at a time.\n",
    "\n",
    "To make this manageable, the script is organized into clearly commented blocks, where each block corresponds to one execution of the bootstrap_ridge() function with a specific subject and embedding. These blocks are initially commented out and labeled with instructions so that you can manually uncomment and run them one by one. This helps prevent memory overload and gives you manual control over the execution flow, allowing you to monitor the progress and intermediate results before proceeding to the next computation.\n",
    "\n",
    "Intermediate outputs such as correlation scores and selected alpha values are saved to disk using compressed .npz files. This ensures that heavy computations do not need to be repeated if the session is interrupted, making the process more fault-tolerant and efficient.\n",
    "\n",
    "Finally, once all models are run, the script includes a visualization section that loads the saved results and helps compare different embeddings based on metrics like mean, median, and top-percentile correlation coefficients. The BoW model, being the most memory-intensive due to its large feature space, is scheduled to run last.\n",
    "\n",
    "Overall, the structure of this codebase is designed to be memory-conscious, reproducible, and well-suited for iterative analysis in a constrained computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total common stories across embeddings and both subjects: 0\n",
      "Training stories: 0; Testing stories: 0\n",
      "Processing subject2 with BoW embeddings...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No overlapping stories found!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 202\u001b[39m\n\u001b[32m    199\u001b[39m         plot_cc_distribution(voxel_ccs, subject, best_emb)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 180\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    178\u001b[39m emb_dict = embeddings[emb_name]\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m embeddings...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m X_train, Y_train = \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_stories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmri_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m X_test, Y_test = prepare_data(test_stories, emb_dict, fmri_data)\n\u001b[32m    182\u001b[39m metrics = train_and_evaluate(X_train, Y_train, X_test, Y_test, subject, emb_name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mprepare_data\u001b[39m\u001b[34m(stories, embedding_dict, fmri_dict)\u001b[39m\n\u001b[32m     66\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Story \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is missing in embeddings or fMRI data.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_list) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Y_list) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo overlapping stories found!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m X = np.concatenate(X_list, axis=\u001b[32m0\u001b[39m)\n\u001b[32m     70\u001b[39m Y = np.concatenate(Y_list, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No overlapping stories found!"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from ridge_utils.ridge import bootstrap_ridge\n",
    "import gc  # For garbage collection\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# Parameters for ridge regression\n",
    "alphas = np.logspace(1, 3, 10)\n",
    "nboots = 10\n",
    "chunklen = 20\n",
    "nchunks = 5\n",
    "corrmin = 0.2\n",
    "\n",
    "# =============== STEP 1: RUN RIDGE ON Word2Vec embedding ===============\n",
    "\n",
    "# Load data for Word2Vec and subject2 (repeat for subject3)\n",
    "X_train = joblib.load(f\"{DATA_DIR}/X_train_Word2Vec.joblib\")\n",
    "X_test = joblib.load(f\"{DATA_DIR}/X_test_Word2Vec.joblib\")\n",
    "Y_train = np.load(f\"{DATA_DIR}/Y_train_subject2.npz\")[\"arr_0\"]\n",
    "Y_test = np.load(f\"{DATA_DIR}/Y_test_subject2.npz\")[\"arr_0\"]\n",
    "\n",
    "print(\"Loaded Word2Vec and subject2 data.\")\n",
    "\n",
    "# Run bootstrap ridge regression (return_wt=False)\n",
    "_, corrs_w2v, valphas_w2v, _, _ = bootstrap_ridge(\n",
    "    Rstim=X_train, Rresp=Y_train, \n",
    "    Pstim=X_test, Presp=Y_test, \n",
    "    alphas=alphas, \n",
    "    nboots=nboots, chunklen=chunklen, nchunks=nchunks, \n",
    "    corrmin=corrmin, single_alpha=False, return_wt=False\n",
    ")\n",
    "\n",
    "# Save the results\n",
    "np.savez_compressed(f\"{DATA_DIR}/ridge_corrs_Word2Vec_subject2.npz\", corrs_w2v, valphas_w2v)\n",
    "\n",
    "# Clean up memory\n",
    "del X_train, X_test, Y_train, Y_test, corrs_w2v, valphas_w2v\n",
    "gc.collect()\n",
    "\n",
    "# ===============================================\n",
    "# Repeat the same STEP 1 block for subject3\n",
    "# ===============================================\n",
    "\n",
    "# =============== STEP 2: RUN RIDGE ON GloVe embedding ===============\n",
    "\n",
    "# Load data for GloVe and subject2\n",
    "X_train = joblib.load(f\"{DATA_DIR}/X_train_GloVe.joblib\")\n",
    "X_test = joblib.load(f\"{DATA_DIR}/X_test_GloVe.joblib\")\n",
    "Y_train = np.load(f\"{DATA_DIR}/Y_train_subject2.npz\")[\"arr_0\"]\n",
    "Y_test = np.load(f\"{DATA_DIR}/Y_test_subject2.npz\")[\"arr_0\"]\n",
    "\n",
    "print(\"Loaded GloVe and subject2 data.\")\n",
    "\n",
    "_, corrs_glove, valphas_glove, _, _ = bootstrap_ridge(\n",
    "    Rstim=X_train, Rresp=Y_train,\n",
    "    Pstim=X_test, Presp=Y_test,\n",
    "    alphas=alphas,\n",
    "    nboots=nboots, chunklen=chunklen, nchunks=nchunks,\n",
    "    corrmin=corrmin, single_alpha=False, return_wt=False\n",
    ")\n",
    "\n",
    "np.savez_compressed(f\"{DATA_DIR}/ridge_corrs_GloVe_subject2.npz\", corrs_glove, valphas_glove)\n",
    "\n",
    "del X_train, X_test, Y_train, Y_test, corrs_glove, valphas_glove\n",
    "gc.collect()\n",
    "\n",
    "# ===============================================\n",
    "# Repeat the same STEP 2 block for subject3\n",
    "# ===============================================\n",
    "\n",
    "# =============== STEP 3: RUN RIDGE ON BoW embedding (largest dataset, last) ===============\n",
    "\n",
    "# Load data for BoW and subject2\n",
    "X_train = joblib.load(f\"{DATA_DIR}/X_train_BoW.joblib\")\n",
    "X_test = joblib.load(f\"{DATA_DIR}/X_test_BoW.joblib\")\n",
    "Y_train = np.load(f\"{DATA_DIR}/Y_train_subject2.npz\")[\"arr_0\"]\n",
    "Y_test = np.load(f\"{DATA_DIR}/Y_test_subject2.npz\")[\"arr_0\"]\n",
    "\n",
    "print(\"Loaded BoW and subject2 data.\")\n",
    "\n",
    "_, corrs_bow, valphas_bow, _, _ = bootstrap_ridge(\n",
    "    Rstim=X_train, Rresp=Y_train,\n",
    "    Pstim=X_test, Presp=Y_test,\n",
    "    alphas=alphas,\n",
    "    nboots=nboots, chunklen=chunklen, nchunks=nchunks,\n",
    "    corrmin=corrmin, single_alpha=False, return_wt=False\n",
    ")\n",
    "\n",
    "np.savez_compressed(f\"{DATA_DIR}/ridge_corrs_BoW_subject2.npz\", corrs_bow, valphas_bow)\n",
    "\n",
    "del X_train, X_test, Y_train, Y_test, corrs_bow, valphas_bow\n",
    "gc.collect()\n",
    "\n",
    "# ===============================================\n",
    "# Repeat the same STEP 3 block for subject3\n",
    "# ===============================================\n",
    "\n",
    "# =============== STEP 4: Compare Performance of Embeddings ===============\n",
    "\n",
    "# Load previously computed correlation results\n",
    "corrs_w2v_sub2 = np.load(f\"{DATA_DIR}/ridge_corrs_Word2Vec_subject2.npz\")[\"arr_0\"]\n",
    "corrs_glove_sub2 = np.load(f\"{DATA_DIR}/ridge_corrs_GloVe_subject2.npz\")[\"arr_0\"]\n",
    "corrs_bow_sub2 = np.load(f\"{DATA_DIR}/ridge_corrs_BoW_subject2.npz\")[\"arr_0\"]\n",
    "\n",
    "# Mean correlation across voxels\n",
    "print(\"Word2Vec mean CC:\", np.mean(corrs_w2v_sub2))\n",
    "print(\"GloVe mean CC:\", np.mean(corrs_glove_sub2))\n",
    "print(\"BoW mean CC:\", np.mean(corrs_bow_sub2))\n",
    "\n",
    "# Choose best embedding based on mean CC (highest)\n",
    "# (Perform similar analysis for median, top 1%, top 5%, etc.)\n",
    "\n",
    "# =============== STEP 5: Detailed Analysis for Best Embedding ===============\n",
    "\n",
    "# Assuming GloVe is the best embedding (example)\n",
    "best_corrs = corrs_glove_sub2\n",
    "\n",
    "# Plot distribution of CC across voxels\n",
    "plt.hist(best_corrs, bins=100)\n",
    "plt.xlabel(\"Correlation Coefficient (CC)\")\n",
    "plt.ylabel(\"Number of Voxels\")\n",
    "plt.title(\"Distribution of CC for best embedding (GloVe)\")\n",
    "plt.show()\n",
    "\n",
    "# Perform stability analysis and interpretation according to PCS\n",
    "# (Additional analysis as per instructions can be performed here)\n",
    "\n",
    "# Clean up after everything\n",
    "del corrs_w2v_sub2, corrs_glove_sub2, corrs_bow_sub2, best_corrs\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat214",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
